{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df=pd.read_csv(\"TwitterApr22_timestamp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eTVYk2bhvnWx",
    "outputId": "6f1906cb-2fd4-45ff-9359-7b810d80dc18"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "#wordpunct_tokenize = WordPunctTokenizer().tokenize\n",
    "def cleaner(tweet):\n",
    "    s=tweet.find('@')\n",
    "    tweet=tweet.replace(tweet[:s],\"\")\n",
    "    tweet=re.sub(r'^.*?@', '@', tweet)  \n",
    "    tweet=re.sub(r'@[A-Za-z0-9]+',\"\",tweet)#Remove @ sign\n",
    "    tweet=re.sub(r'#','',tweet)\n",
    "    tweet=re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    tweet=re.sub('RT[\\s]+','',tweet)\n",
    "    tweet=re.sub('https?:\\/\\/\\S+','',tweet)#Remove http links\n",
    "    #tweet=\" \".join(w for w in nltk.wordpunct_tokenize(tweet) \\\n",
    "    #          if w.lower() in words or not w.isalpha())\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = ''.join(c for c in tweet if c not in emoji.UNICODE_EMOJI) #Remove Emojis\n",
    "    tweet  = ''.join([char for char in tweet if char not in string.punctuation])\n",
    "    return tweet\n",
    "train_df['Description'] = train_df['Description'].map(lambda x: cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzsEtUIdvuj7",
    "outputId": "b553b1f4-795c-4a28-9e12-6f3a367a8f63"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "def lemma(data):\n",
    "    for index,row in data.iterrows():\n",
    "        filter_sentence=''\n",
    "        sentence=row['Description']\n",
    "       \n",
    "        words=nltk.word_tokenize(sentence)\n",
    "        words=[w for w in words if w not in stopwords]\n",
    "        for word in words:\n",
    "            filter_sentence=filter_sentence+\" \"+str(lemmatizer.lemmatize(word)).lower()\n",
    "        data.loc[index,'Description']=filter_sentence    \n",
    "#lemma(test_df)\n",
    "lemma(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBvdsRJGttrr"
   },
   "outputs": [],
   "source": [
    "c_id=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ku-Xnl2ettrs",
    "outputId": "a8832806-1780-4d1e-c956-4ec2446640f3"
   },
   "outputs": [],
   "source": [
    "for j,i in enumerate(train_df['description']):\n",
    "    x=14\n",
    "    #print(j)\n",
    "    #if j==6:\n",
    "    #  break\n",
    "    \n",
    "        \n",
    "    if 'vaccine' in i or 'health' in i or 'covid19' in i or 'covid' in i or 'pandemic' in i  or 'surgery' in i or 'abnormal' in i or 'symptom' in i or 'nutritionist' in i or 'nutrition' in i or 'pain' in i or 'depression' in i or ' surgically ' in i or 'medic' in i or ' drug ' in i or ' cancer ' in i  or ' injuries ' in i or ' disability ' in i or 'therapy' in i or 'anxiety' in i or ' stress ' in i:\n",
    "        x=5\n",
    "        #print('category_5')\n",
    "        #print(i)\n",
    "        #print(j)\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Sports' in i  or 'sport' in i or 'league' in i or 'fitness' in i  or 'athletics' in i or 'Exercise' in i or 'exercise' in i:\n",
    "        x=6\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Food' in i or 'food' in i or ' cafe ' in i or ' drink ' in i or 'dinner' in i or 'cook' in i or 'dish' in i  or 'vegan' in i or 'cook' in i:\n",
    "        x=7\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if ('education' in i   or 'academia' in i or 'insightful' in i  or 'learning' in i  or 'reading' in i or 'study' in i or 'meet-up' in i or 'library' in i or 'family' in i or ' brother ' in i or ' mother ' in i or ' sister ' in i or ' father ' in i  or 'children' in i or 'educators' in i) and(' case ' not in i)  :\n",
    "        x=8\n",
    "        #print(j)\n",
    "        #print(i)\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'poet' in i or 'hobbies' in i or 'writing' in i or 'ballet' in i or 'poetry' in i or 'sketch' in i  or 'dance' in i  or ' craft ' in i:\n",
    "        #print(j)\n",
    "        x=9\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'community' in i  or ' men ' in i or ' black ' in i or 'immigrant' in i or 'immigration' in i or 'migrant' in i or 'gents' in i or 'israel' in i or 'muslim' in i or 'lgbt' in i or ' racist' in i or 'systemic' in i:\n",
    "        #print(j)\n",
    "        x=10\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Zumba' in i or 'zumba' in i:\n",
    "        x=11\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if ' perform ' in i or 'Art' in i or ' art ' in i or 'Cultural' in i or ' cultural ' in i:\n",
    "        x=12\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Recovery' in i or 'recovery' in i:\n",
    "        x=13\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if ' home ' in i or 'lifestyle' in i:\n",
    "        x=15\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Business' in i or 'business' in i or ' trade ' in i or 'trading' in i or 'finance' in i or 'financial' in i or 'customer' in i or 'entrepreneur' in i or 'market' in i or ' fund ' in i or ' shareholders' in i or 'startup' in i:\n",
    "        x=16\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Auto' in i or ' auto ' in i or 'Boat' in i or ' boat ' in i or 'Air' in i or ' air ' in i or 'Flight' in i or 'flight' in i:\n",
    "        x=17\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Causes' in i or 'causes' in i or 'Charity' in i or 'charity' in i or 'activist' in i or 'helpfulness' in i or ' anti ' in i :\n",
    "        x=18\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if ('film' in i  or ' media ' in i or 'Series' in i  or 'television' in i) and ('social' not in i):\n",
    "        x=19\n",
    "        #print(i)\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'government' in i or 'political' in i or 'politics' in i or 'election' in i or 'voting' in i or ' govern' in i or 'president' in i:\n",
    "        #print(i)\n",
    "        x=20\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Meditation' in i or 'meditation' in i or 'spirituality' in i or 'spiritual'in i or ' mind ' in i or 'mindfullness' in i or 'Mindfullness' in i or 'Mental' in i :\n",
    "        x=21\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if ' exam ' in i or 'school' in i or 'lecture' in i or 'assignment' in i or 'grade' in i or 'student' in i :\n",
    "        x=22\n",
    "        #c_id.append(x)\n",
    "    \n",
    "    if 'scientist' in i or 'science' in i or 'scientific' in i  or 'technology' in i or 'analytics' in i or 'tech' in i or 'research' in i or ' digital ' in i or 'logy' in i or 'computer' in i or 'artificialintelligence' in i or 'artificial intelligence' in i or 'datascience' in i or 'data' in i or 'cyber' in i:\n",
    "        x=23\n",
    "        #print(i)\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Holiday' in i or 'holiday' in i or 'Leave' in i :\n",
    "        x=24\n",
    "        #c_id.append(x)\n",
    "    if  'Yoga' in i or ' yoga ' in i:\n",
    "        x=1\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Music' in i or ' music ' in i or 'Acoustics' in i or ' acoustics ' in i  or ' sing ' in i or ' singing ' in i or ' lyric ' in i or ' spotify ' in i:\n",
    "        x=2\n",
    "        #print(j)\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Travel' in i or ' travel ' in i or 'Trip' in i or ' trip ' in i or 'Vacation' in i or ' vacation ' in i:\n",
    "        x=3\n",
    "        #c_id.append(x)\n",
    "        \n",
    "    if 'Fashion' in i or ' fashion ' in i or 'Styling' in i or ' styling ' in i:\n",
    "        x=4\n",
    "        #c_id.append(x)\n",
    "    print(j)\n",
    "    print(x)\n",
    "    print(i)\n",
    "    c_id.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2VP4yslwcxQ",
    "outputId": "b80580d0-949c-4581-9b5b-12ac76e8c7eb"
   },
   "outputs": [],
   "source": [
    "original_id=train_df['category_id'].values.tolist()\n",
    "len(original_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[',end=\"\")\n",
    "for i in original_id:\n",
    "  print(int(i),end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptmuGkOjy5wy"
   },
   "outputs": [],
   "source": [
    "train_df['category_id']=c_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCxn_2e0zArh"
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('TwitterApr16_cat_id.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFHGzH7GEwrc",
    "outputId": "0d0f3bd6-954d-49ae-ee20-c742b344e9c3"
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for l1,l2 in zip(c_id,original_id):\n",
    "  if l1==l2:\n",
    "    count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65sIJcVpiyJD",
    "outputId": "cad24298-c907-4b18-881f-5ac977503418"
   },
   "outputs": [],
   "source": [
    "for j,i in enumerate(train_df['description']):\n",
    "  #print(i)\n",
    "  if j==371:\n",
    "    #print(i)\n",
    "    if 'vaccine' in i or 'health' in i or 'covid19' in i or 'covid' in i or 'pandemic' in i  or 'surgery' in i or 'abnormal' in i or 'symptom' in i or 'nutritionist' in i or 'nutrition' in i or 'pain' in i or 'depression' in i or ' surgically ' in i or 'medic' in i or ' drug ' in i or ' cancer ' in i  or ' injuries ' in i or ' disability ' in i or 'therapy' in i or 'anxiety' in i or ' stress ' in i:\n",
    "        print(i)\n",
    "       #print(i.find('digital'))\n",
    "       #print(i[])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s='13h calling all african american parents professional and community providers join west fresno christian coalition waking ourselves greater mental wealth selfcare 2021 mon 426 530pm space limited register now 1'\n",
    "#s='13h join gaston porter health improvement centers discussion caribbean community covid19 risk factor hypertension obesity diabetes stress negative outcome one health click to register'\n",
    "#s='procure 6m have signed modular buildings mb2 framework webinar well giving detailed overview framework includes register modularconstruction modularbuildings mmc modernmethodsofconstruction'\n",
    "s='agency 46s next webinar clearing 2021 streamline clearing conversion chatbots register now'\n",
    "health=['vaccine','health','covid19','covid' ,'pandemic','surgery' ,'abnormal','symptom','nutritionist','nutrition','pain','depression',' surgically ','medic',' drug ',' cancer ',' injuries ',' disability ','therapy','anxiety',' stress ' ]\n",
    "community=['community', ' men ',' black ','immigrant','immigration','migrant','gents','israel','muslim','lgbt',' racist','systemic']\n",
    "recovery=['Recovery','recovery']\n",
    "others=[]\n",
    "res=sum(1 for i in others if i in s)\n",
    "c_dictionary=dict.fromkeys(['health','community'],0)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#s='13h join gaston porter health improvement centers discussion caribbean community covid19 risk factor hypertension obesity diabetes stress negative outcome one health click to register'\n",
    "largest_list=[]\n",
    "yoga_list=['Yoga',' yoga ']\n",
    "music_list=['Music',' music ','Acoustics',' acoustics ',' sing ',' singing ', ' lyric ', ' spotify ']\n",
    "travel_list=['Travel',' travel ','Trip',' trip ', 'Vacation',' vacation ']\n",
    "fashion_list=['Fashion' , ' fashion ','Styling' , ' styling ']\n",
    "health_list=['vaccine','health','covid19','covid' ,'pandemic','surgery' ,'abnormal','symptom','nutritionist','nutrition','pain','depression',' surgically ','medic',' drug ',' cancer ',' injuries ',' disability ','therapy','anxiety',' stress ' ]\n",
    "sports_list=['Sports','sport','league' ,'fitness','athletics','Exercise','exercise']\n",
    "food_list=['Food','food' ,' drink ','dinner','cook','dish','vegan','cook','chef']\n",
    "education_list=['education','academia','insightful','learning','reading','study','meet-up','library','family',' brother ',' mother ',' sister ',' father ','children','educators']\n",
    "hobbies_list=['poet','hobbies','writing','ballet','poetry','sketch','dance',' craft ']\n",
    "community_list=['community', ' men ',' black ','immigrant','immigration','migrant','gents','israel','muslim','lgbt',' racist','systemic']\n",
    "zumba_list=['Zumba','zumba']\n",
    "perform_arts_list=[' perform ','Art',' art ','Cultural',' cultural ']\n",
    "recovery_list=['Recovery','recovery']\n",
    "others=[]\n",
    "home_lifestyle_list=[' home ','lifestyle']\n",
    "business_list=['Business','business',' trade ','trading','finance','financial','customer','entrepreneur','market',' fund ',' shareholders','startup']\n",
    "auto_boat_list=['Auto',' auto ','Boat',' boat ','Air',' air ','Flight','flight']\n",
    "charity_causes_list=['Causes','causes','Charity','charity','activist','helpfulness',' anti ']\n",
    "film_media_list=['film',' media ','Series','television']\n",
    "government_list=['government','political','politics','election','voting',' govern','president']\n",
    "spirituality_list=['Meditation','meditation','spirituality','spiritual',' mind ','mindfullness','Mindfullness','Mental']\n",
    "school_list=[' exam ','school','assignment','grade','student']\n",
    "science_tech_list=['scientist','science','scientific','technology','analytics','tech','research',' digital ','AI','logy','computer','artificialintelligence','artificial intelligence','datascience','data','cyber']\n",
    "holiday_list=['Holiday','holiday']\n",
    "\n",
    "c_list=[yoga_list, music_list, travel_list, fashion_list, health_list, sports_list, food_list, education_list, hobbies_list, community_list, zumba_list, perform_arts_list, recovery_list, others, home_lifestyle_list, business_list, auto_boat_list, charity_causes_list, film_media_list, government_list, spirituality_list, school_list, science_tech_list, holiday_list]\n",
    "\n",
    "for index,desc in enumerate(train_df['Description']):\n",
    "    #no_of_matches=0\n",
    "    greatest_freq=0\n",
    "    c_id=0\n",
    "    largest_cid=0\n",
    "    for j in c_list:\n",
    "        print(desc)\n",
    "        no_of_matches=0\n",
    "        c_id+=1\n",
    "        no_of_matches=sum(1 for i in j if i in desc)\n",
    "        print(\"no_of_matches\")\n",
    "        print(no_of_matches)\n",
    "        #print(\"c_id\")\n",
    "        #print(c_id)\n",
    "        if greatest_freq<=no_of_matches:\n",
    "            greatest_freq=no_of_matches\n",
    "            largest_cid=c_id\n",
    "        #c_id+=1\n",
    "    print(greatest_freq)\n",
    "    if greatest_freq==0:\n",
    "        largest_cid=14\n",
    "    print(\"####################################final category##########################################\")\n",
    "    print(largest_cid)\n",
    "    largest_list.append(largest_cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "largest_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['category_id']=largest_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('TwitterApr22_cid.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for l1,l2 in zip(largest_list,original_id):\n",
    "    if l1==l2:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(original_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Category_Id_Eesha.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
